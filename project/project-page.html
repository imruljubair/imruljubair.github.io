<!DOCTYPE html>
<html>
<head>
<style>
div {
    margin-top: 100px;
    margin-bottom: 100px;
    margin-left: 100px;
    margin-right: 50px;
    font-size: 80%;
    font-family:Arial;
    text-align: justify;
}
</style>
</head>
<head>
  <title>Projects | Jubair</title>
</head>
<body>

<div>
  <h2>Mohammad Imrul Jubair | Projects</h2>
  <!--<h2>Project Page</h2>-->
  <p>|<a href="/index.html#projects"> back to homepage </a>|</p>
  <br>
  <br>
  <br>
  <a name="tab"></a>
  <h2> DIY Graphic Tab (2021) <hr style="width:100%;text-align:left;margin-left:0"> </h2>
  <p class="lead"> [Completed] </p>
  <p class="lead" >
  
    Everyday, more and more people are turning to online learning, which has altered our traditional classroom method. Recording lectures has always been a normal task for online educators, and it has lately become even more important during the epidemic because actual lessons are still being postponed in several countries. When recording lectures, a graphics tablet is a great substitute for a whiteboard because of its portability and ability to interface with computers. This graphic tablet, however, is too expensive for the majority of instructors. In this paper, we propose a computer vision-based alternative to the graphics tablet for instructors and educators, which functions largely in the same way as a graphic tablet but just requires a pen, paper, and a laptop's webcam. We call it "Do-It-Yourself Graphics Tab" or "DIY Graphics Tab". Our system receives a sequence of images of a person's writing on paper acquired by a camera as input and outputs the screen containing the contents of the writing from the paper. The task is not straightforward since there are many obstacles such as occlusion due to the person's hand, random movement of the paper, poor lighting condition, perspective distortion due to the angle of view, etc. A pipeline is used to route the input recording through our system, which conducts instance segmentation and preprocessing before generating the appropriate output. We also conducted user experience evaluations from the teachers and students, and their responses are examined in this paper. 
<!--<br><br>
   <strong>Student collaborators:</strong> Tashfiq, Jamy, Yousuf & Foisal.-->
   
   <br>
   <br>
   <strong>Outcome:</strong>
   <br>
<ul>
  <li>Paper: ‘DIY graphics tab: A cost­effective alternative to graphics tablet for educators’, AAAI 2022 workshop.[ <a href="http://ai4ed.cc/workshops/aaai2022#:~:text=DIY%20Graphics%20Tab%3A%20A%20Cost%2DEffective%20Alternative%20to%20Graphics%20Tablet%20for%20Educators" ; style="font-weight:bold">link</a> ] [ <a href="https://arxiv.org/abs/2112.03269"; style="font-weight:bold">arXiv</a> ] [ <a href="https://github.com/Arafat-yousuf/DIY-Graphic-Tab_Mask-RCNN"; style="font-weight:bold">code</a> ]
  </li>
  </ul>
  
</p>
<p align="center">
   <img src="material/tab2.JPG" height ="300"> <img src="material/tab3.JPG" height ="480">
  <br>
   <iframe width="560" height="315" src="https://www.youtube.com/embed/3FhcmeKcWqE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
   <iframe width="560" height="315" src="https://www.youtube.com/embed/rkcHIDOOoKQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
  <br>
  <br>
  <br>
  <br>
  <a name="textedit"></a>
  <h2> Bengali Scene Text With Style Retention (2021) <hr style="width:100%;text-align:left;margin-left:0"> </h2>
  <p class="lead"> [Ongoing] </p>
  <p class="lead" >
    We aim to edit Bengali scene text images. The text in a scene image is hard to change without destroying its natural look, background, colour, and style. This transformation has to be done so that both the environment and the text remain the same as before. It becomes even more complicated when the text is in Bengali because the geometric structure of the Bengali alphabet is very complex compared to the English alphabet.
<!--<br><br>
   <strong>Student collaborators:</strong> Dip, Sifat, Fuad & Sajal.-->
</p>
<p align="center">
    <img src="material/textedit.JPG" height ="220">
</p>
</ul>
  <br>
  <br>
  <br>
  <br>
  <a name="bookcover"></a>
  <h2> Book Cover Synthesis (2021) <hr style="width:100%;text-align:left;margin-left:0"> </h2>
  <p class="lead"> [Ongoing] </p>
  <p class="lead" >
    Covers are the face of a book. Designing a book cover has been the job of humans for a long time. In this project, we explore ways to create a book cover from its summary using machine learning.
<!--<br><br>
   <strong>Student collaborators:</strong> Emadul, Faraz, Jarin & Niloy.-->
</p>
<p align="center">
    <img src="material/bookcover.JPG" height ="180">
</p>
</ul>
  <br>
  <br>
  <br>
  <br>
  <a name="jamdani"></a>
  <h2> Artificial Jamdani Artist (2020) <hr style="width:100%;text-align:left;margin-left:0"></h2>
  <p class="lead"> [Completed] </p>
  <p class="lead">
    Jamdani is the strikingly patterned textile heritage of Bangladesh. The exclusive geometric motifs woven on the fabric are the most attractive part of this craftsmanship having a remarkable influence on textile and fine art. In this paper, we have developed a technique based on Generative Adversarial Network that can learn to generate entirely new Jamdani patterns from a collection of Jamdani motifs that we assembled, the newly formed motifs can mimic the appearance of the original designs. Users can input the skeleton of a desired pattern in terms of rough strokes and our system finalizes the input by generating the complete motif which follows the geometric structure of real Jamdani ones. To serve this purpose, we collected and preprocessed a dataset containing a large number of Jamdani motifs images from authentic sources via fieldwork and applied a state-of-the-art method called pix2pix on it. To the best of our knowledge, this dataset is currently the only available dataset of Jamdani motifs in digital format for computer vision research. Our experimental results of the pix2pix model on this dataset show satisfactory outputs of computer-generated images of Jamdani motifs and we believe that our work will open a new avenue for further research.
<!--<br><br>
   <strong>Student collaborators:</strong> <a href="https://www.linkedin.com/in/shawon-tanvir/">MD Tanvir Rouf Shawon</a>, Raihan, Shifa & Shusmoy.-->
<br>
<br>
<strong>Outcome:</strong>
<br>
<ul>
<li>Paper: ‘Jamdani Motif Generation using Conditional GAN’, ICCIT 2020.[ <a href="https://ieeexplore.ieee.org/document/9392654" ; style="font-weight:bold">link</a> ] [ <a href="material/jam.pdf"; style="font-weight:bold">slides</a> ] [ <a href="https://github.com/raihan-tanvir/generative-jamdani"; style="font-weight:bold">supplementary materials</a> ] 
</li><br>
<li>Selected for Bangladesh Hi-Tech Park Authority & IEB's <a href="https://www.unibatorbd.org/results/#:~:text=Jamdani%20motif%20generation%20using%20conditional%20generative%20adversarial%20network"><strong>UNIBATOR</strong></a> program</li> <br>
<li>Featured in the news portal - <strong>United News Bangladesh (UNB)</strong> <a href="https://unb.com.bd/m/category/Tech/generative-jamdani-a-trailblazing-research-on-jamdani-motif-generation-by-aust-students/96110?fbclid=IwAR1-iMzywgOnlkaFluTXSq5nFNUae1KcUHt6HkMgE2he-LIPc7pVKPVvvBA">[see here]</a>.</li>
</ul>
  </p>
<p align="center">
    <img src="material/jam.png" height ="250"> <img src="material/jam2.png" height ="250">
	<iframe width="460" height="275" src="https://www.youtube.com/embed/sZwqE2wFpFs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
  <br>
  <br>
  <br>
  <br>
  <a name="i2i"></a>
  <h2> Image to Image Attire Transfer (2020)<hr style="width:100%;text-align:left;margin-left:0"></h2>
  <p class="lead"> [Completed] </p>
  <p class="lead">

    Virtual trial room is a lucrative tool for online-based attire shopping. Developing such a system is very challenging as it requires robustness from a user's point of view. In this paper, we present a technique for image-to-image attire transfer using Generative Adversarial Networks (GAN) and image processing methods that can transfer the clothing from a person's image to another person's image. Our system takes two images: (1) a full-length image of the user, and (2) an image of another person with a target clothing. Our method then produces a new image of the user with the targeted outfit while keeping the shape, pose, action, and identity of the user unchanged. For this purpose, we exploited the Liquid Warping GAN for domain transfer and U-Net with Grab-cut for segmentation. We illustrate the results of our work in this paper and the outcomes show that our approach performs very satisfactorily for image-to-image attire transfer. We believe this work will contribute to the clothing sector of our e-commerce system by making the shopping smoother, as it will be able to transfer a human model's outfit to a buyer's body in the image and thus helping him/her in deciding a suitable product to purchase.
<!--	<br>
<br>
   <strong>Student collaborators:</strong> <a href="https://www.linkedin.com/in/syed-sanzam-a3911b1a3/">Sanzam Sayed</a>, Dipta, Nabil & Faisal.-->
<br>
<br>
<strong>Outcome:</strong>
<br>
<ul>
<li>Paper: ‘Image to Image Attire Transfer for Virtual Trial Room’, ICCIT 2020.[ <a href="https://ieeexplore.ieee.org/abstract/document/9392671" ; style="font-weight:bold">link</a> ] [ <a href="material/i2i.pdf"; style="font-weight:bold">slides</a> ]
</li>
</ul>
  </p>
<p align="center">
    <img src="material/i2i.png" height ="300">
	<iframe width="500" height="300" src="https://www.youtube.com/embed/xeydk6Ukwx0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
 <br>
  <br>
  <br>
  <a name="shapes2toon"></a>
  <h2> Synthesizing Cartoons from Basic Geometric Primitives (2020) <hr style="width:100%;text-align:left;margin-left:0"> </h2>
  <p class="lead"> [Ongoing] </p>
  <p class="lead" >
    In cartoon animations, geometry plays a significant role in drawing cartoons. Drawing cartoon characters as a combination of geometric shapes is the most straightforward technique of all. Basic geometric shapes such as triangles, rectangles, circles, and ellipses can be amalgamated to make structures of a particular entity, which later can be utilized to draw that entity. The essence of drawing cartoons as a combination of simple geometric shapes lies in the simplicity of thinking. The vast majority of the individuals may not draw a mickey mouse consummately or like an expert artist; however, they can draw a simple mickey mouse with the combination of geometric shapes like circles, ellipses, etc. Moreover, physically and mentally challenged children usually don’t have the same outlook and capability to think as professional artists. Our work can be helpful in drawing apparatus for themselves as well as for the other kids. To solve this complex problem and simplify it, we propose a method, Shapes2toon, based on Image to Image Translation with conditional generative adversarial network model for translating the simple geometric representation of a cartoon character to a natural cartoon character. For this purpose, we developed a javascript-based drawing tool named “Toon Shape Tracer” to trace the shapes manually from the cartoon characters and built a dataset.
<!--<br><br>
   <strong>Student collaborators:</strong> <a href="https://www.linkedin.com/in/simantaturja/">Simanta Deb Turja</a>, Dipu, Shovon & Zahid.-->
<br>
<br>
</p>
<p align="center">
    <img src="material/shapes2toon.JPG" height ="230">
</p>
  <br>
  <br>
  <br>
  <br>
  <a name="bdsl"></a>
  <h2> Bangladeshi sign language detection using Deep Learning (2018) <hr style="width:100%;text-align:left;margin-left:0"></h2>
  <p class="lead"> [Completed] </p>
  <p class="lead">
    Bangladeshi Sign Language (BdSL) is a commonly used medium of communication for the hearing-impaired people in Bangladesh. A real-time BdSL interpreter with no controlled lab environment has a broad social impact and an interesting avenue of research as well. Also, it is a challenging task due to the variation in different subjects (age, gender, color, etc.), complex features, and similarities of signs and clustered backgrounds. However, the existing dataset for BdSL classification task is mainly built in a lab friendly setup which limits the application of powerful deep learning technology. In this paper, we introduce a dataset named BdSL36 which incorporates background augmentation to make the dataset versatile and contains over four million images belonging to 36 categories. Besides, we annotate about 40,000 images with bounding boxes to utilize the potentiality of object detection algorithms. Furthermore, several intensive experiments are performed to establish the baseline performance of our BdSL36. Moreover, we employ beta testing of our classifiers at the user level to justify the possibilities of real-world application with this dataset. We believe our BdSL36 will expedite future research on practical sign letter classification. We make the datasets and all the pre-trained models available for further researcher.
	<!--<br><br>
<br>
   <strong>Student collaborators:</strong> <a href="https://oishee30.github.io/OisheeBinteyHoque/">Oishee Hoque</a>, Md. Saiful Islam, Al-Farabi Akash and Alvin Paulson.-->
<br>
<br>
<strong>Outcomes:</strong>
<br>
<ul>
<li>Paper: ‘BdSL36: A Dataset for Bangladeshi Sign Letters Recognition’, ACCV 2020 (Workshop paper).
[ <a href="https://openaccess.thecvf.com/content/ACCV2020W/MLCSA/html/Hoque_BdSL36_A_Dataset_for_Bangladeshi_Sign_Letters_Recognition_ACCVW_2020_paper.html" ; style="font-weight:bold">pdf</a> ] [ <a href="https://accv2020.github.io/miniconf/workshop_poster_4007.html" ; style="font-weight:bold">presentation at ACCV2020</a> ] [ <a href="https://drive.google.com/drive/folders/1f9sLKK6D86tZH5celUQzUdZI0UtiRfGQ" ; style="font-weight:bold">supplementary materials</a> ] [ <a href="https://drive.google.com/drive/folders/1f9sLKK6D86tZH5celUQzUdZI0UtiRfGQ"; style="font-weight:bold">Our BDSL36 dataset</a> ] </li><br>
<li>Paper: ‘Real Time Bangladeshi Sign Language Detection using Faster R-CNN’, ICIET 2018.
[ <a href="https://arxiv.org/abs/1811.12813" ; style="font-weight:bold">arXiv</a> ] [ <a href="material/bdsl_slides.pdf"; style="font-weight:bold">slides</a> ] [ <a href="https://github.com/imruljubair/bdslimset"; style="font-weight:bold">Our BdSLImSet dataset</a> ]</li>
</ul>
</ul>
  </p>
<p align="center">
    <img src="material/rcnn.png">
    <img src="material/bdsl.png">
  <br>
  <iframe width="460" height="275" src="https://www.youtube.com/embed/zIWTa7TiXNc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  <iframe width="400" height="275" src="https://www.youtube.com/embed/J2BPew-ASpw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <br>
  <iframe width="360" height="275" src="https://www.youtube.com/embed/8NLwOpQCmW0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  <img src="material/bdsl_comp.JPG" height ="230">
</p>
  <br>
  <br>
  <br>
  <br>

  <a name="cartoon"></a>
  <h2>Toon2real: Cartoon to Photorealistic image synthesis using GAN (2018) <hr style="width:100%;text-align:left;margin-left:0"></h2>
  <p class="lead"> [Completed] </p>
  <p class="lead">
    In terms of Image-to-image translation, Generative Adversarial Networks (GANs) has achieved great success even when it is used in the unsupervised dataset. In this work, we aim to translate cartoon images to photo-realistic images using GAN. We apply several state-of-the-art models to perform this task; however, they fail to perform good quality translations. We observe that the shallow difference between these two domains causes this issue. Based on this idea, we propose a method based on CycleGAN model for image translation from cartoon domain to photo-realistic domain. To make our model efficient, we implemented Spectral Normalization which added stability in our model. We demonstrate our experimental results and show that our proposed model has achieved the lowest Fréchet Inception Distance score and better results compared to another state-of-the-art technique, UNIT. 
<!--<br>
<br>
   <strong>Student collaborators:</strong> <a href="https://www.linkedin.com/in/arf111/">K M Arefeen Sultan</a>, Labiba Kanij, Nahidul Islam and Syed Hossain Khan.-->
<br>
<br>
<strong>Outcomes:</strong>
<br>
<ul>
<li>Paper: 'toon2real: Translating Cartoon Images to Realistic Images', ICTAI 2020 (Short paper). [ <a href="https://ieeexplore.ieee.org/abstract/document/9288181/authors#authors"; style="font-weight:bold">link</a> ] [<a href = "https://arxiv.org/abs/2102.01143" ; style="font-weight:bold">arXiv</a>]</li>
<br>
<li>Paper: ‘Cartoon-to-real: An Approach to Translate Cartoon to Realistic Images using GAN’ (student poster), ICIET 2018.
[ <a href="https://arxiv.org/abs/1811.11796" ; style="font-weight:bold">arXiv</a> ] [ <a href="material/cartoon_poster.pdf"  ; style="font-weight:bold">poster</a> ]</li>
<br>
<li>1st Runner-up: Poster presentation at MINDSPARKS 2019.</li><br>
</ul>
<p align="center">
<!--  <img src="material/t2r.png">
  <br>-->
  <img src="material/t2rall.png" width="500" height="250">
<iframe width="460" height="275" src="https://www.youtube.com/embed/ySdTsOcYgyg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
<br>
<br>
<br>
<br>

<a name="whymyface"></a>
<h2>WhyMyFace: <i>Wh</i><small>at it is in </small><i>y</i><small>our </small><i>M</i><small>ind is in </small><i>y</i><small>our </small><i>Face</i>! (2018) <hr style="width:100%;text-align:left;margin-left:0"></h2>
<p class="lead"> [Completed] </p>
<p class="lead">
  Faces and their expressions are one of the potent subjects for digital images. Detecting emotions from images is an ancient task in the field of computer vision; however, performing its reverse -- synthesizing facial expressions from images -- is quite new. Such operations of regenerating images with different facial expressions, or altering an existing expression in an image require the Generative Adversarial Network (GAN). In this paper, we aim to change the facial expression in an image using GAN, where the input image with an initial expression (i.e., happy) is altered to a different expression (i.e., disgusted) for the same person. We used StarGAN techniques on a modified version of the MUG dataset to accomplish this objective. Moreover, we extended our work further by remodeling facial expressions in an image indicated by the emotion from a given text. As a result, we applied a Long Short-Term Memory (LSTM) method to extract emotion from the text and forwarded it to our expression-altering module. As a demonstration of our working pipeline, we also create an application prototype of a blog that regenerates the profile picture with different expressions based on the user's textual emotion.
  <!--<br>
<br>
  <strong>Student collaborators:</strong> Md. Amir Hamza, <a href="https://www.linkedin.com/in/masud-164/">Md. Masud Rana</a>, Ahnaf Tahseen and Fahim Ahsan Khan.-->
<br><br>
<strong>Outcomes:</strong>
 <br>
<ul>
<li>Paper: ‘Altering Facial Expression Based on Textual Emotion’, VISAPP2022 (Short paper) [ <a href="https://arxiv.org/abs/2112.01454"; style="font-weight:bold">arXiv</a> ]</li><br>
</ul>
<p align="center">
<img src="material/whymy2.JPG" height = "350"> <img src="material/whymy.JPG" height ="320">
<iframe width="560" height="315" src="https://www.youtube.com/embed/u3Jw35rtLNk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
<br>
<br>
<br>
<br>

<a name="icomaps"></a>
<h2>Icosahedral Maps for a Multiresolution Representation of Earth Data (2014 - 2016) <hr style="width:100%;text-align:left;margin-left:0"></h2>
<p class="lead"> [Completed] </p>
<p class="lead">
The icosahedral non-hydrostatic (ICON) model is a digital Earth model based on an icosahedral representation and used for numerical weather prediction. In this paper, we introduce icosahedral maps that are designed to fit the geometry of different cell configurations in the ICON model. These maps represent the connectivity information in ICON in a highly structured two-dimensional hexagonal representation that can be adapted to fit different cell configurations. Our maps facilitate the execution of a multiresolution analysis on the ICON model. We demonstrate this by applying a hexagonal version of the discrete wavelet transform in conjunction with our icosahedral maps to decompose ICON data to different levels of detail and to compress it via a thresholding of the wavelet coefficients.
<ul>
The project makes the following contributions:
<br>
<br>
<ul>
<li>We introduce <strong>"Icosahedral Maps"</strong>, an efficient mapping technique to capture the connectivity information for all cell-types in ICON’s soup structure and store it in a highly structured representation. This is an extension to the atlas of connectivity maps (ACM) data structure and represents the Earth’s surface as 2D hexagonal grids. We provide the necessary grid traversal scheme called – <strong>"Hexagonal Fan"</strong> – that suitably records the connectivity of all cell types in ICON in a common hexagonal representation.</li><br>
<li>We demonstrate, for ICON data, the decomposition to and reconstruction from different levels-of-detail using a function-based dyadic hexagonal wavelet scheme. Given the icosahedral maps, the wavelet filters associated with the <strong>hexagonal discrete wavelet transform</strong> are efficiently applied via 2D convolution, upsampling, and downsampling operations. The benefit of having such maps is that the same wavelet can be used for all cell-types.</li>
</ul>
<br>
Similar models to ICON – in terms of grid layout and structure – are the MPAS and the NICAM model. This makes the methods developed for ICON in this project almost directly applicable to MPAS and NICAM as well.
<br>
<br>
<strong>Outcomes:</strong>
<br>
<br>
<ul>
<li>Paper: "Icosahedral Maps for a Multiresolution Representation of Earth Data", VMV2016.
[ <a href="https://dl.acm.org/citation.cfm?id=3056928" ; style="font-weight:bold">link</a> ] [ <a href="papers/jubairVMV2016.pdf" ; style="font-weight:bold">paper</a> ]</li><br>
<li>Thesis: "Icosahedral Maps for a Multiresolution Representation of Earth Data", Department of Computer Science, University of Calgary, 2016.
[ <a href="https://prism.ucalgary.ca/handle/11023/3527" ; style="font-weight:bold">link</a> ] [ <a href="http://visagg.cpsc.ucalgary.ca/sites/default/files/vmv%2Bdkrz.pdf" ; style="font-weight:bold">slides</a> ]</li><br>
<li>Paper: "Multiresolution Visualization of Digital Earth Data via Hexagonal Box Spline Wavelets", VIS2015.
[ <a href="https://ieeexplore.ieee.org/abstract/document/7429508/" ; style="font-weight:bold">link</a> ] [ <a href="papers/SciVisPoster2015.pdf" ; style="font-weight:bold">poster</a> ]</li>
</ul>

</ul>
</p>
<p align="center">
<br>
  <img src="material/thesis.png" height ="250">  <img src="material/thesis2.png" height ="150"> <img src="material/icosa.png" height ="150">
</p>
<br>
<br>
<br>
<br>

<a name="acm4icon"></a>
<h2>Implementing Atlas of Connectivity Maps for ICON Grid (2014) <hr style="width:100%;text-align:left;margin-left:0"></h2>
<p class="lead"> [Completed] </p>
<p class="lead">
  This project was a part of my MSc thesis. The goal of this project is to impelement <a href="https://doi.org/10.1016/j.cag.2013.09.003">the Atlas of Connectivity Maps</a> on <a href="https://www.dwd.de/EN/research/weatherforecasting/num_modelling/01_num_weather_prediction_modells/icon_description.html">ICON (Icosahedral Nonhydrostatic)</a> data. I developed a system that unfold the semiregular grids of the ICON model and map its connectivity information into 2D arrays corresponding to 2D regular pateches. I created a GUI to explore the ICON vertices and connectivity information interactively. The project was succefully completed as SciVis course project in fall 2014.
<br>
 <br>
  <strong>Outcome:</strong>
<br>
<ul>
<li>The project was later exteneded and multiresolution filters are applied on the 2D pateches to explore different levels-of-details (LoDs). The project was presented as a poster in CPSC Industrial Day 2015, University of Calgary.
[ <a href="material/Implementing Atlas of Connectivity Maps for ICON Grid.pdf" ; style="font-weight:bold">project report</a> ]  [ <a href="material/CPSC 601 - Project Final Presentation.pdf" ; style="font-weight:bold">slides</a> ]  [ <a href="material/industrial_day_poster_jubair_(latest).pdf" ; style="font-weight:bold">poster</a> ]  [ <a href="https://github.com/imruljubair/SciVis-Course-Project-Fall-2014" ; style="font-weight:bold">code</a> ]</li>
</ul>
</ul>
</p>
<p align="center">
<br>
  <img src="material/screenshot.png" height ="300">
</p>
<br>
<br>
<br>
<br>

<a name="glslicon"></a>
<h2>GLSL Impelementation of ICON Data Visualization (2015) <hr style="width:100%;text-align:left;margin-left:0"></h2>
<p class="lead"> [Completed] </p>
<p class="lead">
This is a part of my CPSC 691 Rendering course project. In this project, I visualized the triangle meshe from ICON (Icosahedral Nonhydrostatic) dataset using GLSL. In order to visualize, I used my favourite Parula colormap. I followed multipassing method to render triangle meshe with the edges. 
[<a href="https://github.com/imruljubair/Visualization-using-GLSL" ; style="font-weight:bold">code</a>]
</ul>
</p>
<p align="center">
<br>
<img src="material/glsl.png" height ="250">  <img src="material/glsl2.png" height ="250">
</p>
<br>
<p>|<a href="/project/project-page.html#";  style="font-weight:bold"> Jump to top </a>| or |<a href="/index.html#projects";  style="font-weight:bold"> Back to homepage </a>|</p>
</div>
</body>
</html>
